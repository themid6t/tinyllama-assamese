{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to preprocess both the available datasets\n",
    "- train-01.txt  (high-quality wayy longer sentences)\n",
    "- train-02.txt  (moderate-quality wayy shorter sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 1024  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/tinyllama-bnb-4bit\")\n",
    "tokenizer.model_max_length = MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_files = {\n",
    "    \"train1\": \"datasets/training/train-01.txt\",\n",
    "    \"train2\": \"datasets/training/train-02.txt\",\n",
    "}\n",
    "dataset = load_dataset(\"text\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Methods of splitting the dataset\n",
    "1. simple way, truncating data to max-seq-len\n",
    "2. using sliding window and less truncation, preserving context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_split(examples):  # Use plural for batched input\n",
    "    flat_texts = []\n",
    "    for text in examples[\"text\"]:  # Iterate over the list of texts in the batch\n",
    "        tokenized = tokenizer(text, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "        input_ids = tokenized[\"input_ids\"][0].tolist()\n",
    "        chunks = [input_ids[i:i + MAX_SEQ_LENGTH] for i in range(0, len(input_ids), MAX_SEQ_LENGTH)]\n",
    "        flat_texts.extend([tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks])\n",
    "    return {\"text\": flat_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_chunking_no_padding(examples, max_seq_length=1024, overlap=512):\n",
    "    \"\"\"\n",
    "    Tokenizes and splits texts into overlapping chunks without padding.\n",
    "    \n",
    "    Args:\n",
    "        examples: Dictionary containing \"text\" field with batch of texts.\n",
    "        max_seq_length: The length of each chunk.\n",
    "        overlap: Number of tokens to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with tokenized and overlapping chunks.\n",
    "    \"\"\"\n",
    "    flat_texts = []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        # Tokenize without truncation\n",
    "        tokenized = tokenizer(text, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "        input_ids = tokenized[\"input_ids\"][0].tolist()\n",
    "\n",
    "        # Sliding window chunking\n",
    "        step = max_seq_length - overlap\n",
    "        chunks = [input_ids[i:i + max_seq_length] for i in range(0, len(input_ids), step)]\n",
    "\n",
    "        # Decode without padding\n",
    "        flat_texts.extend([tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks])\n",
    "\n",
    "    return {\"text\": flat_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each split\n",
    "print(\"Processing train1...\")\n",
    "processed_train1 = dataset[\"train1\"].map(sliding_window_chunking_no_padding, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing train2...\")\n",
    "processed_train2 = dataset[\"train2\"].map(sliding_window_chunking_no_padding, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed = DatasetDict({\n",
    "    \"processed_train1\": processed_train1,\n",
    "    \"processed_train2\": processed_train2,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize processed dataset and check stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and get lengths\n",
    "def get_sequence_lengths(examples):\n",
    "    # Tokenize with return_length=True, no truncation/padding yet\n",
    "    tokenized = tokenizer(examples[\"text\"], return_length=True, truncation=False)\n",
    "    return {\"length\": tokenized[\"length\"]}\n",
    "\n",
    "# Apply the function to the dataset\n",
    "length_dataset = dataset_processed.map(get_sequence_lengths, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lengths for each split\n",
    "train1_lengths = length_dataset[\"processed_train1\"][\"length\"]\n",
    "train2_lengths = length_dataset[\"processed_train2\"][\"length\"]\n",
    "\n",
    "# Combine lengths for overall analysis\n",
    "all_lengths = train1_lengths + train2_lengths\n",
    "\n",
    "# Print lengths\n",
    "print(\"Train1 lengths:\", len(train1_lengths))\n",
    "print(\"Train2 lengths:\", len(train2_lengths))\n",
    "print(\"All lengths:\", len(all_lengths))\n",
    "\n",
    "# Compute statistics of both and combined\n",
    "train1_array = np.array(train1_lengths)\n",
    "train2_array = np.array(train2_lengths)\n",
    "all_array = np.array(all_lengths)\n",
    "\n",
    "train1_stats = {\n",
    "    \"min\": np.min(train1_array),\n",
    "    \"max\": np.max(train1_array),\n",
    "    \"mean\": np.mean(train1_array),\n",
    "    \"median\": np.median(train1_array),\n",
    "    \"percentile_90\": np.percentile(train1_array, 90),\n",
    "    \"percentile_95\": np.percentile(train1_array, 95),\n",
    "    \"percentile_99\": np.percentile(train1_array, 99),\n",
    "}\n",
    "\n",
    "train2_stats = {\n",
    "    \"min\": np.min(train2_array),\n",
    "    \"max\": np.max(train2_array),\n",
    "    \"mean\": np.mean(train2_array),\n",
    "    \"median\": np.median(train2_array),\n",
    "    \"percentile_90\": np.percentile(train2_array, 90),\n",
    "    \"percentile_95\": np.percentile(train2_array, 95),\n",
    "    \"percentile_99\": np.percentile(train2_array, 99),\n",
    "}\n",
    "\n",
    "all_stats = {\n",
    "    \"min\": np.min(all_array),\n",
    "    \"max\": np.max(all_array),\n",
    "    \"mean\": np.mean(all_array),\n",
    "    \"median\": np.median(all_array),\n",
    "    \"percentile_90\": np.percentile(all_array, 90),\n",
    "    \"percentile_95\": np.percentile(all_array, 95),\n",
    "    \"percentile_99\": np.percentile(all_array, 99),\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSequence Length Statistics of train-01:\")\n",
    "for key, value in train1_stats.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nSequence Length Statistics of train-02:\")\n",
    "for key, value in train2_stats.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nSequence Length Statistics of all:\")\n",
    "for key, value in all_stats.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some cool plots, yaay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# # Simulated sequence length arrays (replace these with your actual data)\n",
    "# train1_lengths = np.random.normal(867, 150, 78495)  # Replace with actual lengths\n",
    "# train2_lengths = np.random.normal(140, 50, 177647)  # Replace with actual lengths\n",
    "# all_lengths = np.concatenate([train1_lengths, train2_lengths])\n",
    "\n",
    "# --- Plot Settings ---\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Dataset Sequence Length Analysis', fontsize=20)\n",
    "\n",
    "# --- Histogram + Normal Distribution ---\n",
    "def plot_histogram_with_normal(ax, data, title):\n",
    "    sns.histplot(data, bins=50, kde=False, ax=ax, color='skyblue', stat='density', label='Actual Distribution')\n",
    "    \n",
    "    # Fit and plot a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    x = np.linspace(min(data), max(data), 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    \n",
    "    ax.plot(x, p, 'r-', label=f'Normal Fit (μ={mu:.2f}, σ={std:.2f})')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "\n",
    "# --- Boxplot ---\n",
    "def plot_boxplot(ax, data, title):\n",
    "    sns.boxplot(x=data, ax=ax, color='lightblue')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "\n",
    "# --- KDE Plot ---\n",
    "def plot_kde(ax, data, title):\n",
    "    sns.kdeplot(data, fill=True, ax=ax, color='lightgreen')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "\n",
    "# --- Combined Plotting ---\n",
    "datasets = [\n",
    "    (train1_lengths, \"Train-01\"),\n",
    "    (train2_lengths, \"Train-02\"),\n",
    "    (all_lengths, \"Combined\")\n",
    "]\n",
    "\n",
    "for i, (data, name) in enumerate(datasets):\n",
    "    # Histogram + Normal\n",
    "    plot_histogram_with_normal(axes[i, 0], data, f\"{name} - Histogram with Normal Fit\")\n",
    "    \n",
    "    # Boxplot\n",
    "    plot_boxplot(axes[i, 1], data, f\"{name} - Boxplot\")\n",
    "    \n",
    "    # KDE Plot\n",
    "    plot_kde(axes[i, 2], data, f\"{name} - KDE Plot\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# --- Normal Distributions Config ---\n",
    "mean_center = 1024\n",
    "std_devs = [100, 300, 500]  # Different spreads\n",
    "x = np.linspace(0, 1500, 1000)  # X-axis range\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Actual Dataset Distribution\n",
    "sns.kdeplot(train1_lengths, label='Train-01', fill=True, color='skyblue', alpha=0.6)\n",
    "sns.kdeplot(train2_lengths, label='Train-02', fill=True, color='lightgreen', alpha=0.6)\n",
    "sns.kdeplot(all_lengths, label='Combined', fill=True, color='coral', alpha=0.6)\n",
    "\n",
    "# Multiple Normal Distributions\n",
    "scaling_factor = len(all_lengths) * 0.005  # Scale the normal distribution properly\n",
    "for std_dev in std_devs:\n",
    "    normal_dist = norm.pdf(x, mean_center, std_dev)\n",
    "    plt.plot(x, normal_dist * scaling_factor, label=f'Normal Dist. (μ=1024, σ={std_dev})', linewidth=2)\n",
    "\n",
    "# Labels and Legends\n",
    "plt.title('Dataset Distribution vs. Multiple Normal Distributions (MAX_SEQ_LENGTH = 1024)', fontsize=18)\n",
    "plt.xlabel('Sequence Length', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.axvline(mean_center, color='black', linestyle='--', label='Max Seq Length (1024)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plotting your actual data\n",
    "sns.kdeplot(train1_lengths, label='Train-01', fill=True, color='skyblue', alpha=0.6)\n",
    "sns.kdeplot(train2_lengths, label='Train-02', fill=True, color='lightgreen', alpha=0.6)\n",
    "sns.kdeplot(all_lengths, label='Combined', fill=True, color='coral', alpha=0.6)\n",
    "\n",
    "# Multiple Normal Distributions\n",
    "for std_dev in [100, 300, 500]:\n",
    "    normal_dist = norm.pdf(x, mean_center, std_dev)\n",
    "    plt.plot(x, normal_dist * 2000, label=f'Normal Dist. (μ=1024, σ={std_dev})', linewidth=2)\n",
    "\n",
    "# Labels and Legends\n",
    "plt.title('Dataset Distribution vs. Multiple Normal Distributions (MAX_SEQ_LENGTH = 1024)', fontsize=18)\n",
    "plt.xlabel('Sequence Length', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.axvline(mean_center, color='black', linestyle='--', label='Max Seq Length (1024)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Histogram for visualization (requires matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting histograms with individual ranges\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Train1 Histogram\n",
    "axes[0].hist(train1_array, bins=50, range=(0, max(2048, train1_stats[\"max\"])), color='blue', alpha=0.7)\n",
    "axes[0].set_title(\"Train1 Sequence Lengths\")\n",
    "axes[0].set_xlabel(\"Length (tokens)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Train2 Histogram\n",
    "axes[1].hist(train2_array, bins=50, range=(0, max(2048, train2_stats[\"max\"])), color='green', alpha=0.7)\n",
    "axes[1].set_title(\"Train2 Sequence Lengths\")\n",
    "axes[1].set_xlabel(\"Length (tokens)\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# All Array Histogram\n",
    "axes[2].hist(all_array, bins=50, range=(0, max(2048, all_stats[\"max\"])), color='orange', alpha=0.7)\n",
    "axes[2].set_title(\"All Array Sequence Lengths\")\n",
    "axes[2].set_xlabel(\"Length (tokens)\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train1, processed_train2\n",
    "# Save the processed datasets to text formats\n",
    "\n",
    "OUTPUT_DIR = \"datasets/processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Combine the processed train splits\n",
    "combined_train = processed_train1[\"text\"] + processed_train2[\"text\"]\n",
    "\n",
    "# Save as plain text\n",
    "def save_text(filename, texts):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in texts:\n",
    "            f.write(text + \"\\n\\n\")  # Separate samples with double newline\n",
    "\n",
    "# Saving individual sets\n",
    "save_text(f\"{OUTPUT_DIR}/processed_train1.txt\", processed_train1[\"text\"])\n",
    "save_text(f\"{OUTPUT_DIR}/processed_train2.txt\", processed_train2[\"text\"])\n",
    "save_text(f\"{OUTPUT_DIR}/combined_train.txt\", combined_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOPS!! Forgot to create validation set\n",
    "\n",
    "- take 5% from both and combine to form val and combine rest to form train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.05  # 5% of the data from each for validation\n",
    "\n",
    "# Split without shuffling → Keep natural order\n",
    "val_size1 = int(len(processed_train1[\"text\"]) * val_ratio)\n",
    "train1_train = processed_train1[\"text\"][:-val_size1]   # 90% for training\n",
    "train1_val = processed_train1[\"text\"][-val_size1:]     # 10% for validation\n",
    "\n",
    "val_size2 = int(len(processed_train2[\"text\"]) * val_ratio)\n",
    "train2_train = processed_train2[\"text\"][:-val_size2]   # 90% for training\n",
    "train2_val = processed_train2[\"text\"][-val_size2:]     # 10% for validation\n",
    "\n",
    "# Combine datasets (no shuffling, context preserved)\n",
    "combined_train = train1_train + train2_train\n",
    "combined_val = train1_val + train2_val\n",
    "\n",
    "# Save the datasets as plain text\n",
    "def save_text(filename, texts):\n",
    "    \"\"\"Saves list of texts into a plain text file\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in texts:\n",
    "            f.write(text + \"\\n\\n\")  # Separate samples with double newline\n",
    "\n",
    "# Save the final datasets\n",
    "save_text(f\"{OUTPUT_DIR}/train-F.txt\", combined_train)\n",
    "save_text(f\"{OUTPUT_DIR}/val-F.txt\", combined_val)\n",
    "\n",
    "print(f\"Train size: {len(combined_train)} samples\")\n",
    "print(f\"Validation size: {len(combined_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate lines\n",
    "with open(f\"{OUTPUT_DIR}/train-F.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "len(lines), len(set(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_comnined = {\n",
    "    \"train1\": \"datasets/processed/train-F.txt\",\n",
    "    \"train2\": \"datasets/processed/val-F.txt\",\n",
    "}\n",
    "dataset_combined = load_dataset(\"text\", data_files=data_comnined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_dataset_combined = dataset_combined.map(get_sequence_lengths, batched=True)\n",
    "# Extract lengths for each split\n",
    "train_lengths = length_dataset_combined[\"train1\"][\"length\"]\n",
    "val_lengths = length_dataset_combined[\"train2\"][\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_lengths), len(val_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More cool vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔥 Normal Distribution Parameters\n",
    "train_mu, train_sigma = np.mean(train_lengths), np.std(train_lengths)\n",
    "val_mu, val_sigma = np.mean(val_lengths), np.std(val_lengths)\n",
    "\n",
    "# 🔥 Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# X values for the normal distribution\n",
    "x = np.linspace(0, 1024, 1000)\n",
    "\n",
    "# 🔥 Plot for Train Dataset\n",
    "axes[0].plot(x, norm.pdf(x, train_mu, train_sigma), color=\"skyblue\", label=f\"Train (μ={train_mu:.2f}, σ={train_sigma:.2f})\")\n",
    "axes[0].axvline(1024, color='red', linestyle='--', label='Max Seq Length = 1024')\n",
    "axes[0].set_title(\"Normal Distribution: Train Dataset\")\n",
    "axes[0].set_xlabel(\"Sequence Length (tokens)\")\n",
    "axes[0].set_ylabel(\"Probability Density\")\n",
    "axes[0].legend()\n",
    "\n",
    "# 🔥 Plot for Validation Dataset\n",
    "axes[1].plot(x, norm.pdf(x, val_mu, val_sigma), color=\"orange\", label=f\"Validation (μ={val_mu:.2f}, σ={val_sigma:.2f})\")\n",
    "axes[1].axvline(1024, color='red', linestyle='--', label='Max Seq Length = 1024')\n",
    "axes[1].set_title(\"Normal Distribution: Validation Dataset\")\n",
    "axes[1].set_xlabel(\"Sequence Length (tokens)\")\n",
    "axes[1].set_ylabel(\"Probability Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assamese",
   "language": "python",
   "name": "assamese"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
